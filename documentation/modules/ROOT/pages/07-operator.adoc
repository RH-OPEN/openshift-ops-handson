## OpenShift Infrastructure Nodes

インフラストラクチャの分類に分類される OpenShift コンポーネントには、次のものがあります。

* kubernetes and OpenShift control plane services ("masters")
* router
* container image registry
* cluster metrics collection ("monitoring")
* cluster aggregated logging
* service brokers

上記以外のコンテナー/ポッド/コンポーネントを実行しているノードはワーカーと見なされ、サブスクリプションでカバーする必要があります。

### More MachineSet Details
`MachineSets` の演習では、 `MachineSets` を使用し、レプリカの数を変更してクラスターをスケーリングする方法について説明しました。 インフラストラクチャ ノードの場合、特定の Kubernetes ラベルを持つ追加の `Machines` を作成します。 次に、さまざまなインフラストラクチャ コンポーネントを構成して、これらのラベルを持つノードで具体的に実行できます。

[Note]
====
現在、インフラストラクチャ コンポーネントを制御するために使用されるオペレーターは、テイントと容認の使用をすべてサポートしているわけではありません。 これは、インフラストラクチャ ワークロードがインフラストラクチャ ノードに移動することを意味しますが、他のワークロードがインフラストラクチャ ノードでの実行を明確に妨げられるわけではありません。 言い換えれば、すべてのオペレーターに完全な汚染/耐性サポートが実装されるまで、ユーザーのワークロードはインフラストラクチャのワークロードと混ざり合う可能性があります。

テイントと容認は連携して機能し、ワークロードが特定のノードにスケジュールされないようにします。 それらは後で調査されます。
====

これを実現するには、追加の `MachineSets` を作成します。

`MachineSets` がどのように機能するかを理解するには、次を実行します。

これにより、以下の説明のいくつかに沿って進めることができます。

[source,bash,role="execute"]
----
CLUSTERNAME=$(oc get  infrastructures.config.openshift.io cluster  -o jsonpath='{.status.infrastructureName}')
ZONENAME=$(oc get nodes -L topology.kubernetes.io/zone  --no-headers  | awk '{print $NF}' | tail -1)
oc get machineset -n openshift-machine-api -o yaml ${CLUSTERNAME}-worker-${ZONENAME}
----

#### Metadata
`MachineSet` 自体の `metadata` には、 `MachineSet` の名前やさまざまなラベルなどの情報が含まれています。

```YAML
metadata:
  creationTimestamp: 2019-01-25T16:00:34Z
  generation: 1
  labels:
    machine.openshift.io/cluster-api-cluster: 190125-3
    machine.openshift.io/cluster-api-machine-role: worker
    machine.openshift.io/cluster-api-machine-type: worker
  name: 190125-3-worker-us-east-1b
  namespace: openshift-machine-api
  resourceVersion: "9027"
  selfLink: /apis/cluster.k8s.io/v1alpha1/namespaces/openshift-machine-api/machinesets/190125-3-worker-us-east-1b
  uid: 591b4d06-20ba-11e9-a880-068acb199400
```

[Note]
====
 `MachineAutoScaler` が定義されているものをダンプした場合、 `MachineSet` に `annotations` が表示される場合があります。
====

#### Selector
`MachineSet` は `Machines` の作成方法を定義し、`Selector` はどのマシンがセットに関連付けられているかをオペレーターに伝えます。

```YAML
spec:
  replicas: 2
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: 190125-3
      machine.openshift.io/cluster-api-machineset: 190125-3-worker-us-east-1b
```

この場合、クラスタ名は「190125-3」で、セット全体に追加のラベルがあります。

### Template Metadata
`template` は `Machine` をテンプレート化する `MachineSet` の一部です。 `template` 自体にメタデータを関連付けることができます。変更を加えるときは、ここで内容が一致していることを確認する必要があります。

```YAML
  template:
    metadata: {}
      labels:
        machine.openshift.io/cluster-api-cluster: 190125-3
        machine.openshift.io/cluster-api-machine-role: worker
        machine.openshift.io/cluster-api-machine-type: worker
        machine.openshift.io/cluster-api-machineset: 190125-3-worker-us-east-1b
```

#### Template Spec
`template` は `Machine`/`Node` の作成方法を指定する必要があります。 `spec`、より具体的には `providerSpec` には、`Machine` を正しく作成してブートストラップするのに役立つすべての重要な AWS データが含まれていることに気付くでしょう。

この場合、結果のノードが 1 つ以上の特定のラベルを継承するようにします。 上記の例で見たように、ラベルは `metadata` セクションに入ります:

```YAML
  spec:
      metadata:
        creationTimestamp: null
      providerSpec:
        value:
          ami:
            id: ami-08871aee06d13e584
...
```

デフォルトでは、インストーラーが作成する `MachineSets` はノードに追加のラベルを適用しません。

### Defining a Custom MachineSet
既存の `MachineSet` を分析したので、それを作成するためのルールを確認する時が来ました。少なくとも、私たちが行っているような単純な変更については:

1. `providerSpec` は何も変更しないでください
2. `machine.openshift.io/cluster-api-cluster: <clusterid>` のインスタンスを変更しないでください
3. `MachineSet` に一意の `name` を付けます
4. `machine.openshift.io/cluster-api-machineset` のすべてのインスタンスが `name` と一致することを確認します
5. ノードに必要なラベルを `.spec.template.spec.metadata.labels` に追加します
6. `MachineSet` の `name` 参照を変更していても、`subnet` は変更しないでください。

これは複雑に聞こえますが、簡単なスクリプトといくつかの手順を用意して、面倒な作業を自動的に行います。

[source,bash,role="execute"]
----
bash {{ HOME_PATH }}/support/machineset-generator.sh 1 infra 0 | oc create -f -
export MACHINESET=$(oc get machineset -n openshift-machine-api -l machine.openshift.io/cluster-api-machine-role=infra -o jsonpath='{.items[0].metadata.name}')
oc patch machineset $MACHINESET -n openshift-machine-api --type='json' -p='[{"op": "add", "path": "/spec/template/spec/metadata/labels", "value":{"node-role.kubernetes.io/worker":"", "node-role.kubernetes.io/infra":""} }]'
oc scale machineset $MACHINESET -n openshift-machine-api --replicas=3
----

次に、実行してください：
[source,bash,role="execute"]
----
oc get machineset -n openshift-machine-api
----

次のような名前の新しいインフラ セットが表示されます。

```
...
cluster-city-56f8-mc4pf-infra-us-east-2a    1         1                             13s
...
```

インスタンスがまだ起動してブートストラップ中であるため、セット内に準備が整った、または使用可能なマシンはまだありません。 `oc get machine -n openshift-machine-api` をチェックして、インスタンスが最終的にいつ実行を開始するかを確認できます。 次に、 `oc get node` を使用して、実際のノードがいつ結合され、準備ができているかを確認できます。

[Note]
====
`Machine` が準備され、 `Node` として追加されるまでに数分かかる場合があります。
====

[source,bash,role="execute"]
----
oc get nodes
----

```
NAME                                         STATUS   ROLES          AGE     VERSION
ip-10-0-133-134.us-east-2.compute.internal   Ready    infra,worker   8m     v1.16.2
ip-10-0-133-191.us-east-2.compute.internal   Ready    worker         61m    v1.16.2
ip-10-0-136-83.us-east-2.compute.internal    Ready    master         67m    v1.16.2
ip-10-0-138-24.us-east-2.compute.internal    Ready    infra,worker   8m1s   v1.16.2
ip-10-0-139-81.us-east-2.compute.internal    Ready    infra,worker   8m3s   v1.16.2
ip-10-0-152-132.us-east-2.compute.internal   Ready    worker         61m    v1.16.2
ip-10-0-157-139.us-east-2.compute.internal   Ready    master         67m    v1.16.2
ip-10-0-167-9.us-east-2.compute.internal     Ready    worker         61m    v1.16.2
ip-10-0-169-121.us-east-2.compute.internal   Ready    master         67m    v1.16.2
```

どのノードが新しいノードか分からない場合は、 `AGE` 列を見てください。 最年少になります！ また、`ROLES` 列では、新しいノードに `worker` と `infra` の両方の役割があることがわかります。

または、役割ごとにノードをリストすることもできます。
[source,bash,role="execute"]
----
oc get nodes -l node-role.kubernetes.io/infra
----

### Check the Labels
私たちの場合、最も若いノードは `ip-10-0-128-138.us-east-1.compute.internal` という名前だったので、そのラベルが何であるかを尋ねることができます:

[source,bash,role="execute"]
----
YOUNG_INFRA_NODE=$(oc get nodes -l node-role.kubernetes.io/infra  --sort-by=.metadata.creationTimestamp -o jsonpath='{.items[0].metadata.name}')
oc get nodes ${YOUNG_INFRA_NODE} --show-labels | grep --color node-role
----

そして、`LABELS` 列には次のように表示されます。

    beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m5.2xlarge,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=us-east-2,failure-domain.beta.kubernetes.io/zone=us-east-2a,kubernetes.io/arch=amd64,kubernetes.io/hostname=ip-10-0-140-3,kubernetes.io/os=linux,node-role.kubernetes.io/infra=,node-role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos

わかりにくいですが、 `node-role.kubernetes.io/infra` というラベルがあります。

### Add More Machinesets (or scale, or both)
現実的な運用展開では、インフラストラクチャ コンポーネントを保持するために少なくとも 3 つの `MachineSets` が必要です。 ロギング集約ソリューションとサービス メッシュの両方が ElasticSearch をデプロイします。ElasticSearch には、3 つの個別のノードにまたがる 3 つのインスタンスが実際に必要です。 なぜ 3 つの `MachineSets` なのか? 理論的には、異なる AZ に複数の `MachineSets` を配置することで、AWS が AZ を失った場合でも完全に暗くなることはありません。

スクリプトレットで作成した `MachineSet` によって、すでに 3 つのレプリカが作成されているので、今のところ何もする必要はありません。 自分で追加のものを作成しないでください。使用しているアカウントの AWS 制限は意図的に小さくなっています。

### Extra Credit
`openshift-machine-api` プロジェクトには、いくつかの `Pod` があります。 それらの 1 つは、`machine-api-controllers-56bdc6874f-86jnb` のような名前を持っています。 その Pod 内のさまざまなコンテナーで oc logs を使用すると、実際にノードを作成するさまざまなオペレーター ビットが表示されます。

## Quick Operator Background
オペレーターは単なる `Pods` です。 しかし、それらは特別な `Pods` です。 これらは、Kubernetes 環境でアプリケーションをデプロイおよび管理する方法を理解するソフトウェアです。 Operator の機能は、`CustomResourceDefinitions` (`CRD`) と呼ばれる Kubernetes 機能に依存しています。  `CRD` はまさにその名のとおりです。 これらは、基本的に新しいオブジェクトで Kubernetes API を拡張するカスタム リソースを定義する方法です。

Kubernetes で `Foo` オブジェクトを作成/読み取り/更新/削除できるようにしたい場合は、 `Foo` リソースとは何か、およびそれがどのように機能するかを定義する `CRD` を作成します。 その後、`CustomResources` (`CRs`) -- `CRD` のインスタンスを作成できます。

Operator の場合、一般的なパターンは、Operator がその構成について `CRs` を参照し、次に Kubernetes 環境で_操作_して、構成で指定されていることを実行するというものです。 ここで、OpenShift の一部のインフラストラクチャ オペレーターがどのように業務を行っているかを見ていきます。

## Moving Infrastructure Components
いくつかの特別なノードができたので、さまざまなインフラストラクチャ コンポーネントをそれらに移動します。

### Router
OpenShift ルーターは、 `openshift-ingress-operator` と呼ばれる `Operator` によって管理されます。 その `Pod` は `openshift-ingress-operator` プロジェクトに存在します。

[source,bash,role="execute"]
----
oc get pod -n openshift-ingress-operator
----

実際のデフォルト ルーター インスタンスは `openshift-ingress` プロジェクトにあります。  `Pods` を見てください。

[source,bash,role="execute"]
----
oc get pods -n openshift-ingress -o wide
----

そして、次のようなものが表示されます。

```
NAME                              READY   STATUS    RESTARTS   AGE   IP           NODE                                        NOMINATED NODE
router-default-7bc4c9c5cd-clwqt   1/1     Running   0          9h    10.128.2.7   ip-10-0-144-70.us-east-2.compute.internal   <none>
router-default-7bc4c9c5cd-fq7m2   1/1     Running   0          9h    10.131.0.7   ip-10-0-138-38.us-east-2.compute.internal   <none>
```

ルーターが実行されている `Node` を確認します。

[source,bash,role="execute"]
----
ROUTER_POD_NODE=$(oc get pods -n openshift-ingress -o jsonpath='{.items[0].spec.nodeName}')
oc get node ${ROUTER_POD_NODE}
----

 `worker` の役割を持っていることがわかります。

```
NAME                                        STATUS   ROLES    AGE   VERSION
ip-10-0-144-70.us-east-2.compute.internal   Ready    worker   9h    v1.12.4+509916ce1
```

ルーター オペレーターのデフォルト設定では、 `worker` の役割を持つノードを選択します。 しかし、専用のインフラストラクチャ ノードを作成したので、 `infra` の役割を持つノードにルーター インスタンスを配置するようにオペレーターに伝えたいと考えています。

OpenShift ルーター オペレーターは、 `ingresses.config.openshift.io` と呼ばれるカスタム リソース定義 ( `CRD` ) を使用して、クラスターのデフォルトのルーティング サブドメインを定義します。

[source,bash,role="execute"]
----
oc get ingresses.config.openshift.io cluster -o yaml
----

`cluster` オブジェクトは、ルーター オペレーターとマスターによって監視されます。 あなたのものはおそらく次のようになります：

```YAML
apiVersion: config.openshift.io/v1
kind: Ingress
metadata:
  creationTimestamp: 2019-04-08T14:37:49Z
  generation: 1
  name: cluster
  resourceVersion: "396"
  selfLink: /apis/config.openshift.io/v1/ingresses/cluster
  uid: e1ec463c-5a0b-11e9-93e8-028b0fb1636c
spec:
  domain: {{ ROUTE_SUBDOMAIN }}
status: {}
```

個々のルーターのデプロイメントは、 `ingresscontrollers.operator.openshift.io` CRD を介して管理されます。 `openshift-ingress-operator` 名前空間に作成されたデフォルトのものがあります。

[source,bash,role="execute"]
----
oc get ingresscontrollers.operator.openshift.io default -n openshift-ingress-operator -o yaml
----

あなたのものは次のようになります：

```YAML
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: 2019-04-08T14:46:15Z
  finalizers:
  - ingress.openshift.io/ingress-controller
  generation: 2
  name: default
  namespace: openshift-ingress-operator
  resourceVersion: "2056085"
  selfLink: /apis/operator.openshift.io/v1/namespaces/openshift-ingress-operator/ingresscontrollers/default
  uid: 0fac160d-5a0d-11e9-a3bb-02d64e703494
spec: {}
status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: 2019-04-08T14:47:14Z
    status: "True"
    type: Available
  domain: apps.cluster-f4a3.f4a3.openshiftworkshop.com
  endpointPublishingStrategy:
    type: LoadBalancerService
  selector: ingress.operator.openshift.io/ingress-controller-deployment=default
```

ルーター Pod にインフラストラクチャ ノードをヒットするように指示する `nodeSelector` を指定するには、次の構成を適用できます。

[source,bash,role="execute"]
----
oc apply -f {{ HOME_PATH }}/support/ingresscontroller.yaml
----

[Note]
====
`Warning: resource is missing the kubectl.kubernetes.io/last-applied-config` というエラーが表示される場合があります。 これは正常です。`apply` は link:https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#kubectl-apply["3 way diff merge"] を呼び出します。 イングレス コントローラーはインストール時に作成されたばかりなので、「最後に適用された」構成はありません。 そのコマンドを再度実行すると、その警告は表示されません。
====


以下のコマンドを実行します。

[source,bash,role="execute"]
----
oc get pod -n openshift-ingress -o wide
----

[Note]
====
ルーターの移動中にセッションがタイムアウトする場合があります。 セッションを元に戻すには、ページを更新してください。 端末セッションが失われることはありませんが、手動でこのページに戻る必要がある場合があります。
====

速ければ、 `Terminating` または `ContainerCreating` ポッドのいずれかをキャッチできます。 「終了」ポッドは、ワーカー ノードの 1 つで実行されていました。  `Running` のポッドは、最終的に `infra` ロールを持つノードの 1 つになります。

## Registry
レジストリは、同様の `CRD` メカニズムを使用して、オペレーターが実際のレジストリ ポッドをデプロイする方法を構成します。 その CRD は `configs.imageregistry.operator.openshift.io` です。 `nodeSelector` を追加するために `cluster` CR オブジェクトを編集します。 まず、それを見てください：

[source,bash,role="execute"]
----
oc get configs.imageregistry.operator.openshift.io/cluster -o yaml
----

次のようなものが表示されます。

```YAML
apiVersion: imageregistry.operator.openshift.io/v1
kind: Config
metadata:
  creationTimestamp: "2019-08-06T13:57:22Z"
  finalizers:
  - imageregistry.operator.openshift.io/finalizer
  generation: 2
  name: cluster
  resourceVersion: "13218"
  selfLink: /apis/imageregistry.operator.openshift.io/v1/configs/cluster
  uid: 1cb6272a-b852-11e9-9a54-02fdf1f6ca7a
spec:
  defaultRoute: false
  httpSecret: fff8bb0952d32e0aa56adf0ac6f6cf5267e0627f7b42e35c508050b5be426f8fd5e5108bea314f4291eeacc0b95a2ea9f842b54d7eb61522238f2a2dc471f131
  logging: 2
  managementState: Managed
  proxy:
    http: ""
    https: ""
    noProxy: ""
  readOnly: false
  replicas: 1
  requests:
    read:
      maxInQueue: 0
      maxRunning: 0
      maxWaitInQueue: 0s
    write:
      maxInQueue: 0
      maxRunning: 0
      maxWaitInQueue: 0s
  storage:
    s3:
      bucket: image-registry-us-east-2-0a598598fc1649d8b96ed91a902b982c-1cbd
      encrypt: true
      keyID: ""
      region: us-east-2
      regionEndpoint: ""
status:
...
```

次のコマンドを実行すると:

[source,bash,role="execute"]
----
oc patch configs.imageregistry.operator.openshift.io/cluster -p '{"spec":{"nodeSelector":{"node-role.kubernetes.io/infra": ""}}}' --type=merge
----

目的の `nodeSelector` を追加するために、レジストリ CR の「.spec」を変更します。

[Note]
====
現時点では、イメージ レジストリはそのオペレーター用に別のプロジェクトを使用していません。 演算子とオペランドの両方が `openshift-image-registry` プロジェクトに格納されています。
====

パッチ コマンドを実行すると、レジストリ ポッドがインフラ ノードに移動されていることがわかります。 レジストリーは `openshift-image-registry` プロジェクトにあります。 以下を十分に迅速に実行した場合：

[source,bash,role="execute"]
----
oc get pod -n openshift-image-registry
----

古いレジストリ ポッドが終了し、新しいレジストリ ポッドが開始されることがあります。 レジストリは S3 バケットによってサポートされているため、新しいレジストリ ポッド インスタンスがどのノードに到達するかは問題ではありません。 API を介してオブジェクト ストアと通信しているため、そこに保存されている既存の画像には引き続きアクセスできます。

また、デフォルトのレプリカ数は 1 であることに注意してください。実際の環境では、可用性、ネットワーク スループット、またはその他の理由で、これをスケールアップしたい場合があります。

レジストリが到達したノード (ルーターのセクションを参照) を見ると、現在はインフラ ワーカーで実行されていることがわかります。

最後に、イメージ レジストリの構成の `CRD` は名前空間ではなく、クラスター スコープであることに注意してください。 OpenShift クラスターごとに 1 つの内部/統合レジストリーしかありません。

## Monitoring
クラスター監視オペレーターは、Prometheus+Grafana+AlertManager クラスター監視スタックの状態のデプロイと管理を担当します。 クラスタの初期インストール時にデフォルトでインストールされます。 そのオペレーターは、`openshift-monitoring` プロジェクトの `ConfigMap` を使用して、監視スタックの動作に関するさまざまな調整可能変数と設定を設定します。

次の `ConfigMap` 定義は、モニタリング ソリューションをインフラストラクチャ ノードに再デプロイするように構成します。

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |+
    alertmanagerMain:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    prometheusK8s:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    prometheusOperator:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    grafana:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    k8sPrometheusAdapter:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    kubeStateMetrics:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    telemeterClient:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
```

インストールの一部として作成される `ConfigMap` はありません。 これがないと、オペレーターはデフォルト設定を想定します。 `ConfigMap` がクラスターで定義されていないことを確認します。

[source,bash,role="execute"]
----
oc get configmap cluster-monitoring-config -n openshift-monitoring
----

以下が表示されます。

```
Error from server (NotFound): configmaps "cluster-monitoring-config" not found
```

次に、オペレーターは、さまざまな監視スタック コンポーネント用にいくつかの `ConfigMap` オブジェクトを作成し、それらも表示できます。

[source,bash,role="execute"]
----
oc get configmap -n openshift-monitoring
----

次のコマンドを使用して、新しい監視構成を作成できます。

[source,bash,role="execute"]
----
oc create -f {{ HOME_PATH }}/support/cluster-monitoring-configmap.yaml
----

監視ポッドが `worker` から `infra`  `Nodes` に移動するのを次のように確認します。

[source,bash,role="execute"]
----
watch 'oc get pod -n openshift-monitoring'
----

もしくは、

[source,bash,role="execute"]
----
oc get pod -w -n openshift-monitoring
----

kbd:[Ctrl+C] を押すと終了できます。
